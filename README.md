# 2018CCF_Contest
基于所选题目的任务，我们搜集了Xgboost和lightgbm两个框架进行学习并将这两个框架作为解题的备选。这两个框架都是基于GBDT算法并进行优化构建而成的。
GBDT：
GBDT的主要思想是利用弱分类器（决策树）迭代训练以得到最优模型，该模型具有训练效果好、不易过拟合等优点。它首先使用训练集和样本真值（即标准答案）训练一棵树，然后使用这棵树预测训练集，得到每个样本的预测值，由于预测值与真值存在偏差，所以二者相减可以得到“残差”。接下来训练第二棵树，此时不再使用真值，而是使用残差作为标准答案。两棵树训练完成后，可以再次得到每个样本的残差，然后进一步训练第三棵树，以此类推。树的总棵数可以人为指定，也可以监控某些指标（例如验证集上的误差）来停止训练。
在预测新样本时，每棵树都会有一个输出值，将这些输出值相加，即得到样本最终的预测值。

Xgboost：
XGBoost是2014年2月诞生的专注于梯度提升算法的机器学习函数库，此函数库因其优良的学习效果以及高效的训练速度而获得广泛的关注。
XGBoost最大的特点在于它能够自动利用CPU的多线程进行并行计算，同时在算法上加以改进提高了精度。XGBoost是GBDT的改进和重要实现，主要在于：
提出稀疏感知(sparsity-aware)算法。
加权分位数快速近似数学学习算法。
缓存访问模式，数据压缩和分片上的实现上的改进。
加入了Shrinkage和列采样，一定程度上防止过拟合。

LightGBM：
LightGBM （Light Gradient Boosting Machine）也是一个实现 GBDT 算法的框架，支持高效率的并行训练，并且具有以下优点：更快的训练速度、更低的内存消耗、更好的准确率、分布式支持，可以快速处理海量数据。
LightGBM 提出的主要原因就是为了解决 GBDT 在海量数据遇到的问题，让 GBDT 可以更好更快地用于工业实践。
概括来说，lightGBM主要有以下特点：基于Histogram的决策树算法、带深度限制的Leaf-wise的叶子生长策略。
Histogram算法：
直方图算法的基本思想：先把连续的浮点特征值离散化成k个整数，同时构造一个宽度为k的直方图。遍历数据时，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。
带深度限制的Leaf-wise的叶子生长策略：
Level-wise过一次数据可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。但实际上Level-wise是一种低效算法，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销，因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。
Leaf-wise则是一种更为高效的策略：每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。因此同Level-wise相比，在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度。
Leaf-wise的缺点：可能会长出比较深的决策树，产生过拟合。因此LightGBM在Leaf-wise之上增加了一个最大深度限制，在保证高效率的同时防止过拟合。
